{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#        *\n",
    "#       /|\\\n",
    "#      /*|O\\\n",
    "#     /*/|\\*\\\n",
    "#    /X/O|*\\X\\\n",
    "#   /*/X/|\\O/*\\\n",
    "#  /X/O/X/|*\\X\\\n",
    "# /O/X/*/O/|X/O\\\n",
    "#        | |\n",
    "#        | |\n",
    "\n",
    "## Whisper with rotary encoder and learned-sinusoid rotary decoder\n",
    "\n",
    "# Standard Libraries\n",
    "import os\n",
    "import re\n",
    "import gzip\n",
    "import csv\n",
    "import warnings\n",
    "from contextlib import contextmanager\n",
    "\n",
    "# Third-Party Libraries\n",
    "import torch\n",
    "import MeCab\n",
    "import librosa\n",
    "import neologdn\n",
    "import evaluate\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union, Optional, Tuple, Iterable\n",
    "\n",
    "# PyTorch Components\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "import torch.profiler as profiler\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "# Datasets and Transformers\n",
    "from datasets import (\n",
    "    load_dataset, \n",
    "    load_from_disk, \n",
    "    concatenate_datasets, \n",
    "    Dataset, \n",
    "    DatasetDict, \n",
    "    Audio, \n",
    "    IterableDataset, \n",
    "    interleave_datasets\n",
    ")\n",
    "from transformers import (\n",
    "    Trainer, \n",
    "    Seq2SeqTrainer, \n",
    "    TrainingArguments, \n",
    "    Seq2SeqTrainingArguments, \n",
    "    WhisperForConditionalGeneration, \n",
    "    WhisperModel, \n",
    "    WhisperTokenizer, \n",
    "    WhisperProcessor, \n",
    "    WhisperConfig, \n",
    "    WhisperFeatureExtractor, \n",
    "    WhisperTokenizerFast, \n",
    "    GenerationConfig, \n",
    "    pipeline, \n",
    "    TrainerControl, \n",
    "    TrainerCallback,\n",
    "    Adafactor, \n",
    "    get_cosine_schedule_with_warmup\n",
    ")\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "from transformers.trainer_pt_utils import IterableDatasetShard\n",
    "\n",
    "# Whisper and Related Libraries\n",
    "from whisper import load_audio, log_mel_spectrogram, pad_or_trim\n",
    "from rotary_embedding_torch import RotaryEmbedding\n",
    "from deepl import translator\n",
    "from whisper_normalizer.basic import BasicTextNormalizer\n",
    "from accelerate import Accelerator\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift\n",
    "\n",
    "# Custom Functions\n",
    "from decoding import decode as decode_function\n",
    "from decoding import detect_language as detect_language_function\n",
    "from transcribe import transcribe as transcribe_function\n",
    "\n",
    "# Optional Import Handling\n",
    "try:\n",
    "    from torch.nn.functional import scaled_dot_product_attention\n",
    "    SDPA_AVAILABLE = True\n",
    "except (ImportError, RuntimeError, OSError):\n",
    "    scaled_dot_product_attention = None\n",
    "    SDPA_AVAILABLE = False\n",
    "\n",
    "# Ignore Warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "warnings.warn = lambda *args, **kwargs: None\n",
    "\n",
    "# Device and Tensor Type\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float32 if torch.cuda.is_available() else torch.float16\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LayerNorm(nn.Module):  # RMSNorm\n",
    "    def __init__(self, dim, unit_offset=False):\n",
    "        super().__init__()\n",
    "        self.unit_offset = unit_offset\n",
    "        self.scale = dim ** 0.5\n",
    "        self.g = nn.Parameter(torch.zeros(dim))\n",
    "        nn.init.constant_(self.g, 1. - float(unit_offset))\n",
    "\n",
    "    def forward(self, x):\n",
    "        gamma = self.g + float(self.unit_offset)\n",
    "        return F.normalize(x, dim=-1) * self.scale * gamma\n",
    "\n",
    "class Linear(nn.Linear):\n",
    "    def forward(self, x):\n",
    "        return F.linear(x, self.weight.to(x.dtype), None if self.bias is None else self.bias.to(x.dtype))\n",
    "\n",
    "class Conv1d(nn.Conv1d):\n",
    "    def _conv_forward(self, x, weight, bias):\n",
    "        return super()._conv_forward(x, weight.to(x.dtype), None if bias is None else bias.to(x.dtype))\n",
    "\n",
    "def sinusofeatures(length, channels, max_timescale=10000):\n",
    "    assert channels % 2 == 0\n",
    "    log_timescale_increment = np.log(max_timescale) / (channels // 2 - 1)\n",
    "    inv_timescales = torch.exp(-log_timescale_increment * torch.arange(channels // 2))\n",
    "    scaled_time = torch.arange(length)[:, np.newaxis] * inv_timescales[np.newaxis, :]\n",
    "    return torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)\n",
    "\n",
    "@contextmanager\n",
    "def disable_sdpa():\n",
    "    prev_state = MultiHeadAttention.use_sdpa\n",
    "    try:\n",
    "        MultiHeadAttention.use_sdpa = False\n",
    "        yield\n",
    "    finally:\n",
    "        MultiHeadAttention.use_sdpa = prev_state\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    use_sdpa = True\n",
    "\n",
    "    def __init__(self, n_state, n_head):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.head_dim = n_state // n_head\n",
    "        self.query = Linear(n_state, n_state)\n",
    "        self.key = Linear(n_state, n_state, bias=False)\n",
    "        self.value = Linear(n_state, n_state)\n",
    "        self.out = Linear(n_state, n_state)\n",
    "        self.rotary_emb = RotaryEmbedding(dim=n_state // n_head)\n",
    "\n",
    "    def forward(self, x, xa=None, mask=None, kv_cache=None):\n",
    "        q = self.query(x)\n",
    "        if kv_cache is None or xa is None or self.key not in kv_cache:\n",
    "            k = self.key(x if xa is None else xa)\n",
    "            v = self.value(x if xa is None else xa)\n",
    "        else:\n",
    "            k = kv_cache[self.key]\n",
    "            v = kv_cache[self.value]\n",
    "\n",
    "        q = self.rotary_emb.rotate_queries_or_keys(q)\n",
    "        k = self.rotary_emb.rotate_queries_or_keys(k)\n",
    "\n",
    "        wv, qk = self.qkv_attention(q, k, v, mask)\n",
    "        return self.out(wv), qk\n",
    "\n",
    "    def qkv_attention(self, q, k, v, mask=None):\n",
    "        n_batch, n_ctx, n_state = q.shape\n",
    "        scale = (n_state // self.n_head) ** -0.25\n",
    "\n",
    "        q = q.view(*q.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)\n",
    "        k = k.view(*k.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)\n",
    "        v = v.view(*v.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)\n",
    "\n",
    "        if SDPA_AVAILABLE and MultiHeadAttention.use_sdpa:\n",
    "            a = scaled_dot_product_attention(q, k, v, is_causal=mask is not None and n_ctx > 1)\n",
    "            out = a.permute(0, 2, 1, 3).flatten(start_dim=2)\n",
    "            qk = None\n",
    "        else:\n",
    "            qk = (q * scale) @ (k * scale).transpose(-1, -2)\n",
    "            if mask is not None:\n",
    "                qk = qk + mask[:n_ctx, :n_ctx]\n",
    "            qk = qk.float()\n",
    "\n",
    "            w = F.softmax(qk, dim=-1).to(q.dtype)\n",
    "            out = (w @ v).permute(0, 2, 1, 3).flatten(start_dim=2)\n",
    "            qk = qk.detach()\n",
    "        return out, qk\n",
    "\n",
    "class ResidualAttentionBlock(nn.Module):\n",
    "    def __init__(self, n_state, n_head, cross_attention=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = MultiHeadAttention(n_state, n_head)\n",
    "        self.attn_ln = LayerNorm(n_state)\n",
    "\n",
    "        self.cross_attention = cross_attention\n",
    "        if self.cross_attention:\n",
    "            self.cross_attn = MultiHeadAttention(n_state, n_head)\n",
    "            self.cross_attn_ln = LayerNorm(n_state)\n",
    "\n",
    "        n_mlp = n_state * 4\n",
    "        self.mlp = nn.Sequential(\n",
    "            Linear(n_state, n_mlp), nn.GELU(), Linear(n_mlp, n_state)\n",
    "        )\n",
    "        self.mlp_ln = LayerNorm(n_state)\n",
    "\n",
    "    def forward(self, x, xa=None, mask=None, kv_cache=None):\n",
    "        x = x + self.attn(self.attn_ln(x), mask=mask, kv_cache=kv_cache)[0]\n",
    "        if self.cross_attention:\n",
    "            x = x + self.cross_attn(self.cross_attn_ln(x), xa, kv_cache=kv_cache)[0]\n",
    "        x = x + self.mlp(self.mlp_ln(x))\n",
    "        return x\n",
    "    \n",
    "class AudioEncoder(nn.Module):\n",
    "    def __init__(self, n_mels, n_ctx, n_state, n_head, n_layer, activation='gelu'):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(n_mels, n_state, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(n_state, n_state, kernel_size=3, stride=2, padding=1)\n",
    "        self.blocks = nn.ModuleList([ResidualAttentionBlock(n_state, n_head) for _ in range(n_layer)])\n",
    "        self.ln_post = LayerNorm(n_state)\n",
    "        self.checkpointing = False\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.checkpointing:\n",
    "            x = checkpoint(self._activate_checkpointed, x, self.conv1)\n",
    "            x = checkpoint(self._activate_checkpointed, x, self.conv2)\n",
    "            for block in self.blocks:\n",
    "                x = checkpoint(self._block_checkpointed, x, block)\n",
    "        else:\n",
    "            x = self._activate(self.conv1(x))\n",
    "            x = self._activate(self.conv2(x))\n",
    "            x = x.permute(0, 2, 1)\n",
    "            for block in self.blocks:\n",
    "                x = block(x)\n",
    "        x = self.ln_post(x)\n",
    "        return x\n",
    "\n",
    "    def _activate(self, layer_output):\n",
    "        if self.activation == 'gelu':\n",
    "            return F.gelu(layer_output)\n",
    "        elif self.activation == 'relu':\n",
    "            return F.relu(layer_output)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "    def _activate_checkpointed(self, x, layer):\n",
    "        output = layer(x)\n",
    "        return self._activate(output)\n",
    "\n",
    "    def _block_checkpointed(self, x, block):\n",
    "        return block(x)\n",
    "\n",
    "    def gradient_checkpointing_enable(self):\n",
    "        self.checkpointing = True\n",
    "\n",
    "    def gradient_checkpointing_disable(self):\n",
    "        self.checkpointing = False\n",
    "\n",
    "\n",
    "\n",
    "# class AudioEncoder(nn.Module):\n",
    "#     def __init__(self, n_mels, n_ctx, n_state, n_head, n_layer):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = nn.Conv1d(n_mels, n_state, kernel_size=3, padding=1)\n",
    "#         self.conv2 = nn.Conv1d(n_state, n_state, kernel_size=3, stride=2, padding=1)\n",
    "#         self.blocks = nn.ModuleList([ResidualAttentionBlock(n_state, n_head) for _ in range(n_layer)])\n",
    "#         self.ln_post = LayerNorm(n_state)\n",
    "#         self.checkpointing = False\n",
    "\n",
    "#     def forward(self, x):\n",
    "    \n",
    "#         x = F.gelu(self.conv1(x))\n",
    "#         x = F.gelu(self.conv2(x))\n",
    "#         x = x.permute(0, 2, 1)\n",
    "\n",
    "#         for block in self.blocks:\n",
    "#             x = checkpoint.checkpoint(block, x)\n",
    "        \n",
    "#         x = self.ln_post(x)\n",
    "#         return x\n",
    "\n",
    "class TextDecoder(nn.Module):\n",
    "    def __init__(self, n_vocab, n_ctx, n_state, n_head, n_layer):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = nn.Embedding(n_vocab, n_state)\n",
    "        self.positional_embedding = LearnedSinusoidalEmbeddings(n_ctx, n_state)\n",
    "        self.rotary_emb = RotaryEmbedding(dim=n_state // n_head)\n",
    "        \n",
    "        self.blocks = nn.ModuleList(\n",
    "            [ResidualAttentionBlock(n_state, n_head, cross_attention=True) for _ in range(n_layer)]\n",
    "        )\n",
    "        self.ln = LayerNorm(n_state)\n",
    "\n",
    "        mask = torch.empty(n_ctx, n_ctx).fill_(-np.inf).triu_(1)\n",
    "        self.register_buffer('mask', mask, persistent=False)\n",
    "\n",
    "    def forward(self, x, xa, kv_cache=None):\n",
    "        offset = next(iter(kv_cache.values())).shape[1] if kv_cache else 0\n",
    "        positions = torch.arange(offset, offset + x.shape[-1], device=x.device)\n",
    "        x = self.token_embedding(x) + self.positional_embedding(positions)\n",
    "        x = x.to(xa.dtype)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, xa, mask=self.mask, kv_cache=kv_cache)\n",
    "\n",
    "        x = self.ln(x)\n",
    "        logits = (x @ torch.transpose(self.token_embedding.weight.to(x.dtype), 0, 1)).float()\n",
    "        return logits\n",
    "\n",
    "def block_forward(block, x, xa, mask, kv_cache):\n",
    "    return block(x, xa, mask=mask, kv_cache=kv_cache)\n",
    "   \n",
    "class LearnedSinusoidalEmbeddings(nn.Module):\n",
    "    def __init__(self, n_ctx, n_state):\n",
    "        super().__init__()\n",
    "        self.n_ctx = n_ctx\n",
    "        self.n_state = n_state\n",
    "\n",
    "        # Initialize with sinusoidal embeddings\n",
    "        sinusoidal_embeddings = sinusofeatures(n_ctx, n_state)\n",
    "        self.positional_embeddings = nn.Parameter(sinusoidal_embeddings)\n",
    "\n",
    "    def forward(self, positions):\n",
    "        position_embeddings = self.positional_embeddings[positions]\n",
    "        return position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomWhisperConfig(WhisperConfig):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_mels = kwargs.get(\"n_mels\", 80)\n",
    "        self.n_audio_ctx = kwargs.get(\"n_audio_ctx\", 1500)\n",
    "        self.n_audio_state = kwargs.get(\"n_audio_state\", 1024)\n",
    "        self.n_audio_head = kwargs.get(\"n_audio_head\", 16)\n",
    "        self.n_audio_layer = kwargs.get(\"n_audio_layer\", 24)\n",
    "        self.n_vocab = kwargs.get(\"n_vocab\", 51865)\n",
    "        self.n_text_ctx = kwargs.get(\"n_text_ctx\", 1500)\n",
    "        self.n_text_state = kwargs.get(\"n_text_state\", 1024)\n",
    "        self.n_text_head = kwargs.get(\"n_text_head\", 15)\n",
    "        self.n_text_layer = kwargs.get(\"n_text_layer\", 24\n",
    "        )\n",
    "\n",
    "class CustomWhisperModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "        self.encoder = AudioEncoder(\n",
    "            config.n_mels,\n",
    "            config.n_audio_ctx,\n",
    "            config.n_audio_state,\n",
    "            config.n_audio_head,\n",
    "            config.n_audio_layer\n",
    "        )\n",
    "        self.decoder = TextDecoder(\n",
    "            config.n_vocab,\n",
    "            config.n_text_ctx,\n",
    "            config.n_text_state,\n",
    "            config.n_text_head,\n",
    "            config.n_text_layer\n",
    "        )\n",
    "\n",
    "        all_heads = torch.zeros(config.n_text_layer, config.n_text_head, dtype=torch.bool)\n",
    "        all_heads[config.n_text_layer // 2:] = True\n",
    "        self.register_buffer('alignment_heads', all_heads.to_sparse(), persistent=False)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, labels=None, **kwargs):\n",
    "        if input_ids is None:\n",
    "            raise ValueError(\"Input IDs cannot be None\")\n",
    "        encoder_outputs = self.encoder(input_ids)\n",
    "        if decoder_input_ids is None:\n",
    "            raise ValueError(\"Decoder input IDs cannot be None\")\n",
    "        decoder_outputs = self.decoder(decoder_input_ids, encoder_outputs)\n",
    "        logits = decoder_outputs.view(-1, decoder_outputs.size(-1))\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            loss = criterion(logits, labels.view(-1))\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "    \n",
    "    # def gradient_checkpointing_enable(self):\n",
    "    #     self.encoder.gradient_checkpointing_enable()\n",
    "    #     # Optionally enable checkpointing for other parts of the model if needed\n",
    "\n",
    "    # def gradient_checkpointing_disable(self):\n",
    "    #     self.encoder.gradient_checkpointing_disable()\n",
    "    #     # Optionally disable checkpointing for other parts of the model if needed\n",
    "\n",
    "\n",
    "    # def gradient_checkpointing_enable(self, **kwargs):\n",
    "    #     # Enable gradient checkpointing for layers that support it\n",
    "    #     for layer in self.children():\n",
    "    #         if hasattr(layer, 'gradient_checkpointing_enable'):\n",
    "    #             layer.gradient_checkpointing_enable(**kwargs)\n",
    "\n",
    "    def embed_audio(self, mel):\n",
    "        return self.encoder(mel)\n",
    "\n",
    "    def logits(self, tokens, audio_features):\n",
    "        return self.decoder(tokens, audio_features)\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    @property\n",
    "    def is_multilingual(self):\n",
    "        return self.config.n_vocab >= 51865\n",
    "\n",
    "    @property\n",
    "    def num_languages(self):\n",
    "        return self.config.n_vocab - 51765 - int(self.is_multilingual)\n",
    "\n",
    "    def install_kv_cache_hooks(self, cache=None):\n",
    "        cache = {**cache} if cache is not None else {}\n",
    "        hooks = []\n",
    "\n",
    "        def save_to_cache(module, _, output):\n",
    "            if module not in cache or output.shape[1] > self.config.n_text_ctx:\n",
    "                cache[module] = output\n",
    "            else:\n",
    "                cache[module] = torch.cat([cache[module], output], dim=1).detach()\n",
    "            return cache[module]\n",
    "\n",
    "        def install_hooks(layer):\n",
    "            if isinstance(layer, MultiHeadAttention):\n",
    "                hooks.append(layer.key.register_forward_hook(save_to_cache))\n",
    "                hooks.append(layer.value.register_forward_hook(save_to_cache))\n",
    "\n",
    "        self.decoder.apply(install_hooks)\n",
    "        return cache, hooks\n",
    "\n",
    "    detect_language = detect_language_function\n",
    "    transcribe = transcribe_function\n",
    "    decode = decode_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the custom configuration\n",
    "config = CustomWhisperConfig(\n",
    "    n_mels=80, \n",
    "    n_audio_ctx=1500, \n",
    "    n_audio_state=1024, \n",
    "    n_audio_head=16, \n",
    "    n_audio_layer=24, \n",
    "    n_vocab=51865, \n",
    "    n_text_ctx=448, \n",
    "    n_text_state=1024, \n",
    "    n_text_head=16, \n",
    "    n_text_layer=24\n",
    "    )\n",
    "\n",
    "# Initialize the custom model\n",
    "model = CustomWhisperModel(config).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferring layer model.encoder.conv1.weight to encoder.conv1.weight\n",
      "Source shape: torch.Size([1024, 80, 3]), Target shape: torch.Size([1024, 80, 3])\n",
      "Transferring layer model.encoder.conv1.bias to encoder.conv1.bias\n",
      "Source shape: torch.Size([1024]), Target shape: torch.Size([1024])\n",
      "Transferring layer model.encoder.conv2.weight to encoder.conv2.weight\n",
      "Source shape: torch.Size([1024, 1024, 3]), Target shape: torch.Size([1024, 1024, 3])\n",
      "Transferring layer model.encoder.conv2.bias to encoder.conv2.bias\n",
      "Source shape: torch.Size([1024]), Target shape: torch.Size([1024])\n",
      "Transferring layer model.decoder.embed_tokens.weight to decoder.token_embedding.weight\n",
      "Source shape: torch.Size([51865, 1024]), Target shape: torch.Size([51865, 1024])\n",
      "Transferring layer model.encoder.layers.0.self_attn.k_proj.weight to encoder.blocks.0.attn.key.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.encoder.layers.0.self_attn.v_proj.weight to encoder.blocks.0.attn.value.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.encoder.layers.0.self_attn.q_proj.weight to encoder.blocks.0.attn.query.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.encoder.layers.0.self_attn.out_proj.weight to encoder.blocks.0.attn.out.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.encoder.layers.0.fc1.weight to encoder.blocks.0.mlp.0.weight\n",
      "Source shape: torch.Size([4096, 1024]), Target shape: torch.Size([4096, 1024])\n",
      "Transferring layer model.encoder.layers.0.fc1.bias to encoder.blocks.0.mlp.0.bias\n",
      "Source shape: torch.Size([4096]), Target shape: torch.Size([4096])\n",
      "Transferring layer model.encoder.layers.0.fc2.weight to encoder.blocks.0.mlp.2.weight\n",
      "Source shape: torch.Size([1024, 4096]), Target shape: torch.Size([1024, 4096])\n",
      "Transferring layer model.encoder.layers.0.fc2.bias to encoder.blocks.0.mlp.2.bias\n",
      "Source shape: torch.Size([1024]), Target shape: torch.Size([1024])\n",
      "Transferring layer model.decoder.layers.0.self_attn.k_proj.weight to decoder.blocks.0.attn.key.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.0.self_attn.v_proj.weight to decoder.blocks.0.attn.value.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.0.self_attn.q_proj.weight to decoder.blocks.0.attn.query.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.0.self_attn.out_proj.weight to decoder.blocks.0.attn.out.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.0.encoder_attn.k_proj.weight to decoder.blocks.0.cross_attn.key.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.0.encoder_attn.v_proj.weight to decoder.blocks.0.cross_attn.value.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.0.encoder_attn.q_proj.weight to decoder.blocks.0.cross_attn.query.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.0.encoder_attn.out_proj.weight to decoder.blocks.0.cross_attn.out.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.0.fc1.weight to decoder.blocks.0.mlp.0.weight\n",
      "Source shape: torch.Size([4096, 1024]), Target shape: torch.Size([4096, 1024])\n",
      "Transferring layer model.decoder.layers.0.fc1.bias to decoder.blocks.0.mlp.0.bias\n",
      "Source shape: torch.Size([4096]), Target shape: torch.Size([4096])\n",
      "Transferring layer model.decoder.layers.0.fc2.weight to decoder.blocks.0.mlp.2.weight\n",
      "Source shape: torch.Size([1024, 4096]), Target shape: torch.Size([1024, 4096])\n",
      "Transferring layer model.decoder.layers.0.fc2.bias to decoder.blocks.0.mlp.2.bias\n",
      "Source shape: torch.Size([1024]), Target shape: torch.Size([1024])\n",
      "Transferring layer model.encoder.layers.1.self_attn.k_proj.weight to encoder.blocks.1.attn.key.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.encoder.layers.1.self_attn.v_proj.weight to encoder.blocks.1.attn.value.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.encoder.layers.1.self_attn.q_proj.weight to encoder.blocks.1.attn.query.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.encoder.layers.1.self_attn.out_proj.weight to encoder.blocks.1.attn.out.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.encoder.layers.1.fc1.weight to encoder.blocks.1.mlp.0.weight\n",
      "Source shape: torch.Size([4096, 1024]), Target shape: torch.Size([4096, 1024])\n",
      "Transferring layer model.encoder.layers.1.fc1.bias to encoder.blocks.1.mlp.0.bias\n",
      "Source shape: torch.Size([4096]), Target shape: torch.Size([4096])\n",
      "Transferring layer model.encoder.layers.1.fc2.weight to encoder.blocks.1.mlp.2.weight\n",
      "Source shape: torch.Size([1024, 4096]), Target shape: torch.Size([1024, 4096])\n",
      "Transferring layer model.encoder.layers.1.fc2.bias to encoder.blocks.1.mlp.2.bias\n",
      "Source shape: torch.Size([1024]), Target shape: torch.Size([1024])\n",
      "Transferring layer model.decoder.layers.1.self_attn.k_proj.weight to decoder.blocks.1.attn.key.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.1.self_attn.v_proj.weight to decoder.blocks.1.attn.value.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.1.self_attn.q_proj.weight to decoder.blocks.1.attn.query.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.1.self_attn.out_proj.weight to decoder.blocks.1.attn.out.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.1.encoder_attn.k_proj.weight to decoder.blocks.1.cross_attn.key.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.1.encoder_attn.v_proj.weight to decoder.blocks.1.cross_attn.value.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.1.encoder_attn.q_proj.weight to decoder.blocks.1.cross_attn.query.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.1.encoder_attn.out_proj.weight to decoder.blocks.1.cross_attn.out.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.1.fc1.weight to decoder.blocks.1.mlp.0.weight\n",
      "Source shape: torch.Size([4096, 1024]), Target shape: torch.Size([4096, 1024])\n",
      "Transferring layer model.decoder.layers.1.fc1.bias to decoder.blocks.1.mlp.0.bias\n",
      "Source shape: torch.Size([4096]), Target shape: torch.Size([4096])\n",
      "Transferring layer model.decoder.layers.1.fc2.weight to decoder.blocks.1.mlp.2.weight\n",
      "Source shape: torch.Size([1024, 4096]), Target shape: torch.Size([1024, 4096])\n",
      "Transferring layer model.decoder.layers.1.fc2.bias to decoder.blocks.1.mlp.2.bias\n",
      "Source shape: torch.Size([1024]), Target shape: torch.Size([1024])\n",
      "Transferring layer model.encoder.layers.2.self_attn.k_proj.weight to encoder.blocks.2.attn.key.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.encoder.layers.2.self_attn.v_proj.weight to encoder.blocks.2.attn.value.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.encoder.layers.2.self_attn.q_proj.weight to encoder.blocks.2.attn.query.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.encoder.layers.2.self_attn.out_proj.weight to encoder.blocks.2.attn.out.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.encoder.layers.2.fc1.weight to encoder.blocks.2.mlp.0.weight\n",
      "Source shape: torch.Size([4096, 1024]), Target shape: torch.Size([4096, 1024])\n",
      "Transferring layer model.encoder.layers.2.fc1.bias to encoder.blocks.2.mlp.0.bias\n",
      "Source shape: torch.Size([4096]), Target shape: torch.Size([4096])\n",
      "Transferring layer model.encoder.layers.2.fc2.weight to encoder.blocks.2.mlp.2.weight\n",
      "Source shape: torch.Size([1024, 4096]), Target shape: torch.Size([1024, 4096])\n",
      "Transferring layer model.encoder.layers.2.fc2.bias to encoder.blocks.2.mlp.2.bias\n",
      "Source shape: torch.Size([1024]), Target shape: torch.Size([1024])\n",
      "Transferring layer model.decoder.layers.2.self_attn.k_proj.weight to decoder.blocks.2.attn.key.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.2.self_attn.v_proj.weight to decoder.blocks.2.attn.value.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.2.self_attn.q_proj.weight to decoder.blocks.2.attn.query.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.2.self_attn.out_proj.weight to decoder.blocks.2.attn.out.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.2.encoder_attn.k_proj.weight to decoder.blocks.2.cross_attn.key.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.2.encoder_attn.v_proj.weight to decoder.blocks.2.cross_attn.value.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.2.encoder_attn.q_proj.weight to decoder.blocks.2.cross_attn.query.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.2.encoder_attn.out_proj.weight to decoder.blocks.2.cross_attn.out.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.2.fc1.weight to decoder.blocks.2.mlp.0.weight\n",
      "Source shape: torch.Size([4096, 1024]), Target shape: torch.Size([4096, 1024])\n",
      "Transferring layer model.decoder.layers.2.fc1.bias to decoder.blocks.2.mlp.0.bias\n",
      "Source shape: torch.Size([4096]), Target shape: torch.Size([4096])\n",
      "Transferring layer model.decoder.layers.2.fc2.weight to decoder.blocks.2.mlp.2.weight\n",
      "Source shape: torch.Size([1024, 4096]), Target shape: torch.Size([1024, 4096])\n",
      "Transferring layer model.decoder.layers.2.fc2.bias to decoder.blocks.2.mlp.2.bias\n",
      "Source shape: torch.Size([1024]), Target shape: torch.Size([1024])\n",
      "Transferring layer model.encoder.layers.3.self_attn.k_proj.weight to encoder.blocks.3.attn.key.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.encoder.layers.3.self_attn.v_proj.weight to encoder.blocks.3.attn.value.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.encoder.layers.3.self_attn.q_proj.weight to encoder.blocks.3.attn.query.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.encoder.layers.3.self_attn.out_proj.weight to encoder.blocks.3.attn.out.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.encoder.layers.3.fc1.weight to encoder.blocks.3.mlp.0.weight\n",
      "Source shape: torch.Size([4096, 1024]), Target shape: torch.Size([4096, 1024])\n",
      "Transferring layer model.encoder.layers.3.fc1.bias to encoder.blocks.3.mlp.0.bias\n",
      "Source shape: torch.Size([4096]), Target shape: torch.Size([4096])\n",
      "Transferring layer model.encoder.layers.3.fc2.weight to encoder.blocks.3.mlp.2.weight\n",
      "Source shape: torch.Size([1024, 4096]), Target shape: torch.Size([1024, 4096])\n",
      "Transferring layer model.encoder.layers.3.fc2.bias to encoder.blocks.3.mlp.2.bias\n",
      "Source shape: torch.Size([1024]), Target shape: torch.Size([1024])\n",
      "Transferring layer model.decoder.layers.3.self_attn.k_proj.weight to decoder.blocks.3.attn.key.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.3.self_attn.v_proj.weight to decoder.blocks.3.attn.value.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.3.self_attn.q_proj.weight to decoder.blocks.3.attn.query.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.3.self_attn.out_proj.weight to decoder.blocks.3.attn.out.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.3.encoder_attn.k_proj.weight to decoder.blocks.3.cross_attn.key.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.3.encoder_attn.v_proj.weight to decoder.blocks.3.cross_attn.value.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.3.encoder_attn.q_proj.weight to decoder.blocks.3.cross_attn.query.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.3.encoder_attn.out_proj.weight to decoder.blocks.3.cross_attn.out.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.3.fc1.weight to decoder.blocks.3.mlp.0.weight\n",
      "Source shape: torch.Size([4096, 1024]), Target shape: torch.Size([4096, 1024])\n",
      "Transferring layer model.decoder.layers.3.fc1.bias to decoder.blocks.3.mlp.0.bias\n",
      "Source shape: torch.Size([4096]), Target shape: torch.Size([4096])\n",
      "Transferring layer model.decoder.layers.3.fc2.weight to decoder.blocks.3.mlp.2.weight\n",
      "Source shape: torch.Size([1024, 4096]), Target shape: torch.Size([1024, 4096])\n",
      "Transferring layer model.decoder.layers.3.fc2.bias to decoder.blocks.3.mlp.2.bias\n",
      "Source shape: torch.Size([1024]), Target shape: torch.Size([1024])\n",
      "Transferring layer model.encoder.layers.4.self_attn.k_proj.weight to encoder.blocks.4.attn.key.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.encoder.layers.4.self_attn.v_proj.weight to encoder.blocks.4.attn.value.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.encoder.layers.4.self_attn.q_proj.weight to encoder.blocks.4.attn.query.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.encoder.layers.4.self_attn.out_proj.weight to encoder.blocks.4.attn.out.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.encoder.layers.4.fc1.weight to encoder.blocks.4.mlp.0.weight\n",
      "Source shape: torch.Size([4096, 1024]), Target shape: torch.Size([4096, 1024])\n",
      "Transferring layer model.encoder.layers.4.fc1.bias to encoder.blocks.4.mlp.0.bias\n",
      "Source shape: torch.Size([4096]), Target shape: torch.Size([4096])\n",
      "Transferring layer model.encoder.layers.4.fc2.weight to encoder.blocks.4.mlp.2.weight\n",
      "Source shape: torch.Size([1024, 4096]), Target shape: torch.Size([1024, 4096])\n",
      "Transferring layer model.encoder.layers.4.fc2.bias to encoder.blocks.4.mlp.2.bias\n",
      "Source shape: torch.Size([1024]), Target shape: torch.Size([1024])\n",
      "Transferring layer model.decoder.layers.4.self_attn.k_proj.weight to decoder.blocks.4.attn.key.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.4.self_attn.v_proj.weight to decoder.blocks.4.attn.value.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.4.self_attn.q_proj.weight to decoder.blocks.4.attn.query.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.4.self_attn.out_proj.weight to decoder.blocks.4.attn.out.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.4.encoder_attn.k_proj.weight to decoder.blocks.4.cross_attn.key.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.4.encoder_attn.v_proj.weight to decoder.blocks.4.cross_attn.value.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.4.encoder_attn.q_proj.weight to decoder.blocks.4.cross_attn.query.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.4.encoder_attn.out_proj.weight to decoder.blocks.4.cross_attn.out.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.4.fc1.weight to decoder.blocks.4.mlp.0.weight\n",
      "Source shape: torch.Size([4096, 1024]), Target shape: torch.Size([4096, 1024])\n",
      "Transferring layer model.decoder.layers.4.fc1.bias to decoder.blocks.4.mlp.0.bias\n",
      "Source shape: torch.Size([4096]), Target shape: torch.Size([4096])\n",
      "Transferring layer model.decoder.layers.4.fc2.weight to decoder.blocks.4.mlp.2.weight\n",
      "Source shape: torch.Size([1024, 4096]), Target shape: torch.Size([1024, 4096])\n",
      "Transferring layer model.decoder.layers.4.fc2.bias to decoder.blocks.4.mlp.2.bias\n",
      "Source shape: torch.Size([1024]), Target shape: torch.Size([1024])\n",
      "Transferring layer model.encoder.layers.5.self_attn.k_proj.weight to encoder.blocks.5.attn.key.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.encoder.layers.5.self_attn.v_proj.weight to encoder.blocks.5.attn.value.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.encoder.layers.5.self_attn.q_proj.weight to encoder.blocks.5.attn.query.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.encoder.layers.5.self_attn.out_proj.weight to encoder.blocks.5.attn.out.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.encoder.layers.5.fc1.weight to encoder.blocks.5.mlp.0.weight\n",
      "Source shape: torch.Size([4096, 1024]), Target shape: torch.Size([4096, 1024])\n",
      "Transferring layer model.encoder.layers.5.fc1.bias to encoder.blocks.5.mlp.0.bias\n",
      "Source shape: torch.Size([4096]), Target shape: torch.Size([4096])\n",
      "Transferring layer model.encoder.layers.5.fc2.weight to encoder.blocks.5.mlp.2.weight\n",
      "Source shape: torch.Size([1024, 4096]), Target shape: torch.Size([1024, 4096])\n",
      "Transferring layer model.encoder.layers.5.fc2.bias to encoder.blocks.5.mlp.2.bias\n",
      "Source shape: torch.Size([1024]), Target shape: torch.Size([1024])\n",
      "Transferring layer model.decoder.layers.5.self_attn.k_proj.weight to decoder.blocks.5.attn.key.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.5.self_attn.v_proj.weight to decoder.blocks.5.attn.value.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.5.self_attn.q_proj.weight to decoder.blocks.5.attn.query.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.5.self_attn.out_proj.weight to decoder.blocks.5.attn.out.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.5.encoder_attn.k_proj.weight to decoder.blocks.5.cross_attn.key.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.5.encoder_attn.v_proj.weight to decoder.blocks.5.cross_attn.value.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.5.encoder_attn.q_proj.weight to decoder.blocks.5.cross_attn.query.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.5.encoder_attn.out_proj.weight to decoder.blocks.5.cross_attn.out.weight\n",
      "Source shape: torch.Size([1024, 1024]), Target shape: torch.Size([1024, 1024])\n",
      "Transferring layer model.decoder.layers.5.fc1.weight to decoder.blocks.5.mlp.0.weight\n",
      "Source shape: torch.Size([4096, 1024]), Target shape: torch.Size([4096, 1024])\n",
      "Transferring layer model.decoder.layers.5.fc1.bias to decoder.blocks.5.mlp.0.bias\n",
      "Source shape: torch.Size([4096]), Target shape: torch.Size([4096])\n",
      "Transferring layer model.decoder.layers.5.fc2.weight to decoder.blocks.5.mlp.2.weight\n",
      "Source shape: torch.Size([1024, 4096]), Target shape: torch.Size([1024, 4096])\n",
      "Transferring layer model.decoder.layers.5.fc2.bias to decoder.blocks.5.mlp.2.bias\n",
      "Source shape: torch.Size([1024]), Target shape: torch.Size([1024])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model = WhisperForConditionalGeneration.from_pretrained('openai/whisper-medium')\n",
    "pretrained_state_dict = pretrained_model.state_dict()\n",
    "\n",
    "model_state_dict = model.state_dict()\n",
    "\n",
    "def transfer_layer(src_name, tgt_name):\n",
    "    if src_name in pretrained_state_dict and tgt_name in model_state_dict:\n",
    "        src_tensor = pretrained_state_dict[src_name]\n",
    "        tgt_tensor = model_state_dict[tgt_name]\n",
    "        print(f'Transferring layer {src_name} to {tgt_name}')\n",
    "        print(f'Source shape: {src_tensor.shape}, Target shape: {tgt_tensor.shape}')\n",
    "        tgt_tensor.copy_(src_tensor)\n",
    "\n",
    "# Transfer convolutional layers\n",
    "transfer_layer('model.encoder.conv1.weight', 'encoder.conv1.weight')\n",
    "transfer_layer('model.encoder.conv1.bias', 'encoder.conv1.bias')\n",
    "transfer_layer('model.encoder.conv2.weight', 'encoder.conv2.weight')\n",
    "transfer_layer('model.encoder.conv2.bias', 'encoder.conv2.bias')\n",
    "\n",
    "# Transfer positional embeddings\n",
    "# transfer_layer('model.encoder.embed_positions.weight', 'encoder.positional_embedding.weight')\n",
    "# transfer_layer('model.decoder.embed_positions.weight', 'decoder.positional_embedding.weight')\n",
    "\n",
    "# Transfer layer norms\n",
    "transfer_layer('model.encoder.layer_norm.weight', 'encoder.ln_post.weight')\n",
    "transfer_layer('model.encoder.layer_norm.bias', 'encoder.ln_post.bias')\n",
    "transfer_layer('model.decoder.layer_norm.weight', 'decoder.ln.weight')\n",
    "transfer_layer('model.decoder.layer_norm.bias', 'decoder.ln.bias')\n",
    "\n",
    "# Transfer token embeddings\n",
    "transfer_layer('model.decoder.embed_tokens.weight', 'decoder.token_embedding.weight') # tokenizer\n",
    "\n",
    "# Transfer encoder and decoder block layers\n",
    "for i in range(6):\n",
    "    transfer_layer(f'model.encoder.layers.{i}.self_attn.k_proj.weight', f'encoder.blocks.{i}.attn.key.weight')\n",
    "    transfer_layer(f'model.encoder.layers.{i}.self_attn.v_proj.weight', f'encoder.blocks.{i}.attn.value.weight')\n",
    "    transfer_layer(f'model.encoder.layers.{i}.self_attn.q_proj.weight', f'encoder.blocks.{i}.attn.query.weight')\n",
    "    transfer_layer(f'model.encoder.layers.{i}.self_attn.out_proj.weight', f'encoder.blocks.{i}.attn.out.weight')\n",
    "    transfer_layer(f'model.encoder.layers.{i}.self_attn_layer_norm.weight', f'encoder.blocks.{i}.attn_ln.weight')\n",
    "    transfer_layer(f'model.encoder.layers.{i}.self_attn_layer_norm.bias', f'encoder.blocks.{i}.attn_ln.bias')\n",
    "    transfer_layer(f'model.encoder.layers.{i}.fc1.weight', f'encoder.blocks.{i}.mlp.0.weight')\n",
    "    transfer_layer(f'model.encoder.layers.{i}.fc1.bias', f'encoder.blocks.{i}.mlp.0.bias')\n",
    "    transfer_layer(f'model.encoder.layers.{i}.fc2.weight', f'encoder.blocks.{i}.mlp.2.weight')\n",
    "    transfer_layer(f'model.encoder.layers.{i}.fc2.bias', f'encoder.blocks.{i}.mlp.2.bias')\n",
    "    transfer_layer(f'model.encoder.layers.{i}.final_layer_norm.weight', f'encoder.blocks.{i}.mlp_ln.weight')\n",
    "    transfer_layer(f'model.encoder.layers.{i}.final_layer_norm.bias', f'encoder.blocks.{i}.mlp_ln.bias')\n",
    "    transfer_layer(f'model.decoder.layers.{i}.self_attn.k_proj.weight', f'decoder.blocks.{i}.attn.key.weight')\n",
    "    transfer_layer(f'model.decoder.layers.{i}.self_attn.v_proj.weight', f'decoder.blocks.{i}.attn.value.weight')\n",
    "    transfer_layer(f'model.decoder.layers.{i}.self_attn.q_proj.weight', f'decoder.blocks.{i}.attn.query.weight')\n",
    "    transfer_layer(f'model.decoder.layers.{i}.self_attn.out_proj.weight', f'decoder.blocks.{i}.attn.out.weight')\n",
    "    transfer_layer(f'model.decoder.layers.{i}.self_attn_layer_norm.weight', f'decoder.blocks.{i}.attn_ln.weight')\n",
    "    transfer_layer(f'model.decoder.layers.{i}.self_attn_layer_norm.bias', f'decoder.blocks.{i}.attn_ln.bias')\n",
    "    transfer_layer(f'model.decoder.layers.{i}.encoder_attn.k_proj.weight', f'decoder.blocks.{i}.cross_attn.key.weight')\n",
    "    transfer_layer(f'model.decoder.layers.{i}.encoder_attn.v_proj.weight', f'decoder.blocks.{i}.cross_attn.value.weight')\n",
    "    transfer_layer(f'model.decoder.layers.{i}.encoder_attn.q_proj.weight', f'decoder.blocks.{i}.cross_attn.query.weight')\n",
    "    transfer_layer(f'model.decoder.layers.{i}.encoder_attn.out_proj.weight', f'decoder.blocks.{i}.cross_attn.out.weight')\n",
    "    transfer_layer(f'model.decoder.layers.{i}.encoder_attn_layer_norm.weight', f'decoder.blocks.{i}.cross_attn_ln.weight')\n",
    "    transfer_layer(f'model.decoder.layers.{i}.encoder_attn_layer_norm.bias', f'decoder.blocks.{i}.cross_attn_ln.bias')\n",
    "    transfer_layer(f'model.decoder.layers.{i}.fc1.weight', f'decoder.blocks.{i}.mlp.0.weight')\n",
    "    transfer_layer(f'model.decoder.layers.{i}.fc1.bias', f'decoder.blocks.{i}.mlp.0.bias')\n",
    "    transfer_layer(f'model.decoder.layers.{i}.fc2.weight', f'decoder.blocks.{i}.mlp.2.weight')\n",
    "    transfer_layer(f'model.decoder.layers.{i}.fc2.bias', f'decoder.blocks.{i}.mlp.2.bias')\n",
    "    transfer_layer(f'model.decoder.layers.{i}.final_layer_norm.weight', f'decoder.blocks.{i}.mlp_ln.weight')\n",
    "    transfer_layer(f'model.decoder.layers.{i}.final_layer_norm.bias', f'decoder.blocks.{i}.mlp_ln.bias')\n",
    "\n",
    "# Load the updated state dict into the custom model\n",
    "model.load_state_dict(model_state_dict)\n",
    "\n",
    "# Now you can proceed with training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "659971281f2c45b087d00400a790a88c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a233ed95f144859b82cf7b4f547113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define processor and tokenizer\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
    "Feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-base\", feature_size=128, do_normalize=True)\n",
    "tokenizer = WhisperTokenizerFast.from_pretrained(\"openai/whisper-base\", padding=\"longest\")\n",
    "\n",
    "metric = evaluate.load(\"cer\")\n",
    "\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    def __init__(self, processor, decoder_start_token_id):\n",
    "        self.processor = processor\n",
    "        self.decoder_start_token_id = decoder_start_token_id\n",
    "\n",
    "    def __call__(self, features):\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        batch[\"input_ids\"] = batch[\"input_features\"]\n",
    "        batch[\"decoder_input_ids\"] = labels_batch[\"input_ids\"]\n",
    "\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")\n",
    "\n",
    "# Prepare the datasets\n",
    "ds_a = load_from_disk(\"D:/proj/datasets/gvjas\")[\"train\"].to_iterable_dataset(num_shards=200).filter(lambda sample: bool(sample[\"sentence\"]))\n",
    "ds_b = load_from_disk(\"D:/proj/datasets/gvjas\")[\"test\"].to_iterable_dataset(num_shards=2).filter(lambda sample: bool(sample[\"sentence\"]))\n",
    "\n",
    "def map_dataset(map):\n",
    "    map[\"input_features\"] = processor.feature_extractor(map[\"audio\"][\"array\"], sampling_rate=map[\"audio\"][\"sampling_rate\"]).input_features[0]\n",
    "    map[\"labels\"] = processor.tokenizer(map[\"sentence\"]).input_ids\n",
    "    return map\n",
    "\n",
    "train = ds_a.map(map_dataset).select_columns([\"input_features\", \"labels\"])\n",
    "test = ds_b.map(map_dataset).select_columns([\"input_features\", \"labels\"])\n",
    "\n",
    "metric = evaluate.load(\"cer\")\n",
    "wakati = MeCab.Tagger(\"-Owakati\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_features = pred.predictions\n",
    "    label_features = pred.label_features\n",
    "    label_features[label_features == -100] = tokenizer.pad_token_id\n",
    "    pred_str = tokenizer.batch_decode(pred_features, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_features, skip_special_tokens=True)\n",
    "    \n",
    "    pred_str_nj = [wakati.parse(pred) for pred in pred_str] \n",
    "    label_str_nj = [wakati.parse(label) for label in label_str] \n",
    "    pred_str_nj = [pred_str_nj[i] for i in range(len(pred_str_nj)) if len(label_str_nj[i]) > 0]\n",
    "    label_str_nj = [\n",
    "        label_str_nj[i]\n",
    "        for i in range(len(label_str_nj))\n",
    "        if len(label_str_nj[i]) > 0]\n",
    "    \n",
    "    pred_str_neo = [neologdn.normalize(pred) for pred in pred_str] \n",
    "    label_str_neo = [neologdn.normalize(label) for label in label_str] \n",
    "    pred_str_neo = [pred_str_neo[i] for i in range(len(pred_str_neo)) if len(label_str_neo[i]) > 0]\n",
    "    label_str_neo = [\n",
    "        label_str_neo[i]\n",
    "        for i in range(len(label_str_neo))\n",
    "        if len(label_str_neo[i]) > 0]\n",
    "    \n",
    "    cer = 100 * metric.compute(predictions=pred_str, references=label_str) # no normalization\n",
    "    cer_mecab = 100 * metric.compute(predictions=pred_str_nj, references=label_str_nj) # mecab normalization\n",
    "    cer_neo = 100 * metric.compute(predictions=pred_str_neo, references=label_str_neo) # \n",
    "    return {\"cer\": cer,  \"cer_mecab\": cer_mecab, \"cer_neo\": cer_neo}#, \"blue\": blue, \"accuracy\": accuracy} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the custom model\n",
    "model = CustomWhisperModel(config, n_mels=n_mels, n_ctx=1500, n_state=1280, n_head=20, n_layer=26, activation='relu').cuda()\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./out\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=1,\n",
    "    learning_rate=6.2e-4,\n",
    "    warmup_steps=100,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=7500,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    eval_steps=50,\n",
    "    fp16=True,\n",
    "    eval_on_start=False,\n",
    "    logging_steps=5,\n",
    "    logging_dir=(\"./logs\"),\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_first_step=True,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    "    hub_private_repo=True,\n",
    "    metric_for_best_model=\"cer\",\n",
    "    predict_with_generate=True,\n",
    "    greater_is_better=False,\n",
    "    generation_max_length=128,\n",
    "    optim = \"adafactor\",\n",
    "    weight_decay=0.002,\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=5e-5,\n",
    "    gradient_accumulation_steps=8,\n",
    "    gradient_checkpointing=True,  # Enable gradient checkpointing\n",
    "    num_train_epochs=3,\n",
    ")\n",
    "\n",
    "# Enable gradient checkpointing for the model\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# To disable gradient checkpointing later if needed\n",
    "model.gradient_checkpointing_disable()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
