{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Whisper with rotary encoder and learned-sinusoid rotary decoder\n",
    "\n",
    "import base64, csv, torchaudio, neologdn, evaluate, MeCab, gzip, numpy as np, torch.nn.functional as F, torch, whisper\n",
    "from contextlib import contextmanager\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Iterable, Optional, Tuple\n",
    "from transformers import TrainingArguments\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import Tensor, nn\n",
    "from rotary_embedding_torch import RotaryEmbedding\n",
    "from transformers import Trainer, WhisperFeatureExtractor, WhisperTokenizerFast\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import Tensor, nn\n",
    "from whisper import load_audio, log_mel_spectrogram, pad_or_trim\n",
    "from typing import Any, Dict, List, Union\n",
    "from torchaudio import datasets\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW, lr_scheduler\n",
    "from torch import optim\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from decoding import decode as decode_function\n",
    "from decoding import detect_language as detect_language_function\n",
    "from transcribe import transcribe as transcribe_function\n",
    "from transformers import Adafactor\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.amp import autocast, GradScaler\n",
    "import torch.profiler as profiler\n",
    "\n",
    "try:\n",
    "    from torch.nn.functional import scaled_dot_product_attention\n",
    "\n",
    "    SDPA_AVAILABLE = True\n",
    "except (ImportError, RuntimeError, OSError):\n",
    "    scaled_dot_product_attention = None\n",
    "    SDPA_AVAILABLE = False\n",
    "\n",
    "import torch\n",
    "from transformers import WhisperForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelDimensions:\n",
    "    n_mels: int\n",
    "    n_audio_ctx: int\n",
    "    n_audio_state: int\n",
    "    n_audio_head: int\n",
    "    n_audio_layer: int\n",
    "    n_vocab: int\n",
    "    n_text_ctx: int\n",
    "    n_text_state: int\n",
    "    n_text_head: int\n",
    "    n_text_layer: int\n",
    "\n",
    "class LayerNorm(nn.Module): #RMSNorm\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        unit_offset = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.unit_offset = unit_offset\n",
    "        self.scale = dim ** 0.5\n",
    "\n",
    "        self.g = nn.Parameter(torch.zeros(dim))\n",
    "        nn.init.constant_(self.g, 1. - float(unit_offset))\n",
    "\n",
    "    def forward(self, x):\n",
    "        gamma = self.g + float(self.unit_offset)\n",
    "        return F.normalize(x, dim = -1) * self.scale * gamma\n",
    "    \n",
    "class Linear(nn.Linear):\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return F.linear(\n",
    "            x,\n",
    "            self.weight.to(x.dtype),\n",
    "            None if self.bias is None else self.bias.to(x.dtype),\n",
    "        )\n",
    "\n",
    "class Conv1d(nn.Conv1d):\n",
    "    def _conv_forward(\n",
    "        self, x: Tensor, weight: Tensor, bias: Optional[Tensor]\n",
    "    ) -> Tensor:\n",
    "        return super()._conv_forward(\n",
    "            x, weight.to(x.dtype), None if bias is None else bias.to(x.dtype)\n",
    "        )\n",
    "    \n",
    "def sinusofeatures(length, channels, max_timescale=10000):\n",
    "    '''Returns sinusofeatures for positional embedding'''\n",
    "    assert channels % 2 == 0\n",
    "    log_timescale_increment = np.log(max_timescale) / (channels // 2 - 1)\n",
    "    inv_timescales = torch.exp(-log_timescale_increment * torch.arange(channels // 2))\n",
    "    scaled_time = torch.arange(length)[:, np.newaxis] * inv_timescales[np.newaxis, :]\n",
    "    return torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)\n",
    "\n",
    "@contextmanager\n",
    "def disable_sdpa():\n",
    "    prev_state = MultiHeadAttention.use_sdpa\n",
    "    try:\n",
    "        MultiHeadAttention.use_sdpa = False\n",
    "        yield\n",
    "    finally:\n",
    "        MultiHeadAttention.use_sdpa = prev_state\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    use_sdpa = True\n",
    "\n",
    "    def __init__(self, n_state: int, n_head: int):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.head_dim = n_state // n_head\n",
    "        self.query = Linear(n_state, n_state)\n",
    "        self.key = Linear(n_state, n_state, bias=False)\n",
    "        self.value = Linear(n_state, n_state)\n",
    "        self.out = Linear(n_state, n_state)\n",
    "        self.rotary_emb = RotaryEmbedding(dim=n_state // n_head)\n",
    "\n",
    "    def forward(self, x: Tensor, xa: Optional[Tensor] = None, mask: Optional[Tensor] = None, kv_cache: Optional[dict] = None):\n",
    "        q = self.query(x)\n",
    "        if kv_cache is None or xa is None or self.key not in kv_cache:\n",
    "            k = self.key(x if xa is None else xa)\n",
    "            v = self.value(x if xa is None else xa)\n",
    "        else:\n",
    "            k = kv_cache[self.key]\n",
    "            v = kv_cache[self.value]\n",
    "\n",
    "        q = self.rotary_emb.rotate_queries_or_keys(q)\n",
    "        k = self.rotary_emb.rotate_queries_or_keys(k)\n",
    "\n",
    "        wv, qk = self.qkv_attention(q, k, v, mask)\n",
    "        return self.out(wv), qk\n",
    "\n",
    "    def qkv_attention(self, q: Tensor, k: Tensor, v: Tensor, mask: Optional[Tensor] = None) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        n_batch, n_ctx, n_state = q.shape\n",
    "        scale = (n_state // self.n_head) ** -0.25\n",
    "\n",
    "        q = q.view(*q.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)\n",
    "        k = k.view(*k.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)\n",
    "        v = v.view(*v.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)\n",
    "\n",
    "        if SDPA_AVAILABLE and MultiHeadAttention.use_sdpa:\n",
    "            a = scaled_dot_product_attention(q, k, v, is_causal=mask is not None and n_ctx > 1)\n",
    "            out = a.permute(0, 2, 1, 3).flatten(start_dim=2)\n",
    "            qk = None\n",
    "        else:\n",
    "            qk = (q * scale) @ (k * scale).transpose(-1, -2)\n",
    "            if mask is not None:\n",
    "                qk = qk + mask[:n_ctx, :n_ctx]\n",
    "            qk = qk.float()\n",
    "\n",
    "            w = F.softmax(qk, dim=-1).to(q.dtype)\n",
    "            out = (w @ v).permute(0, 2, 1, 3).flatten(start_dim=2)\n",
    "            qk = qk.detach()\n",
    "        return out, qk\n",
    "\n",
    "class ResidualAttentionBlock(nn.Module):\n",
    "    def __init__(self, n_state: int, n_head: int, cross_attention: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = MultiHeadAttention(n_state, n_head)\n",
    "        self.attn_ln = LayerNorm(n_state)\n",
    "\n",
    "        self.cross_attention = cross_attention\n",
    "        if self.cross_attention:\n",
    "            self.cross_attn = MultiHeadAttention(n_state, n_head)\n",
    "            self.cross_attn_ln = LayerNorm(n_state)\n",
    "\n",
    "        n_mlp = n_state * 4\n",
    "        self.mlp = nn.Sequential(\n",
    "            Linear(n_state, n_mlp), nn.GELU(), Linear(n_mlp, n_state)\n",
    "        )\n",
    "        self.mlp_ln = LayerNorm(n_state)\n",
    "\n",
    "    def forward(self, x: Tensor, xa: Optional[Tensor] = None, mask: Optional[Tensor] = None, kv_cache: Optional[dict] = None):\n",
    "        x = x + self.attn(self.attn_ln(x), mask=mask, kv_cache=kv_cache)[0]\n",
    "        if self.cross_attention:\n",
    "            x = x + self.cross_attn(self.cross_attn_ln(x), xa, kv_cache=kv_cache)[0]\n",
    "        x = x + self.mlp(self.mlp_ln(x))\n",
    "        return x\n",
    "\n",
    "class AudioEncoder(nn.Module):\n",
    "    def __init__(self, n_mels: int, n_ctx: int, n_state: int, n_head: int, n_layer: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(n_mels, n_state, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(n_state, n_state, kernel_size=3, stride=2, padding=1)\n",
    "        self.blocks = nn.ModuleList([ResidualAttentionBlock(n_state, n_head) for _ in range(n_layer)])\n",
    "        self.ln_post = LayerNorm(n_state)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "#        print(f\"Input shape: {x.shape}\")  # Debug print\n",
    "        x = F.gelu(self.conv1(x))\n",
    "#        print(f\"Shape after conv1: {x.shape}\")  # Debug print\n",
    "        x = F.gelu(self.conv2(x))\n",
    "#        print(f\"Shape after conv2: {x.shape}\")  # Debug print\n",
    "        x = x.permute(0, 2, 1)\n",
    "#        print(f\"Shape after permute: {x.shape}\")  # Debug print\n",
    "\n",
    "        # Apply gradient checkpointing to the blocks\n",
    "        for block in self.blocks:\n",
    "            x = checkpoint.checkpoint(block, x)\n",
    "        \n",
    "        x = self.ln_post(x)\n",
    "#        print(f\"Final output shape: {x.shape}\")  # Debug print\n",
    "        return x\n",
    "\n",
    "def block_forward(block, x, xa, mask, kv_cache):\n",
    "    return block(x, xa, mask=mask, kv_cache=kv_cache)\n",
    "\n",
    "class TextDecoder(nn.Module):\n",
    "    def __init__(self, n_vocab: int, n_ctx: int, n_state: int, n_head: int, n_layer: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = nn.Embedding(n_vocab, n_state)\n",
    "        self.positional_embedding = LearnedSinusoidalEmbeddings(n_ctx, n_state)\n",
    "        self.rotary_emb = RotaryEmbedding(dim=n_state // n_head)\n",
    "        \n",
    "        self.blocks: Iterable[ResidualAttentionBlock] = nn.ModuleList(\n",
    "            [ResidualAttentionBlock(n_state, n_head, cross_attention=True) for _ in range(n_layer)]\n",
    "        )\n",
    "        self.ln = LayerNorm(n_state)\n",
    "\n",
    "        mask = torch.empty(n_ctx, n_ctx).fill_(-np.inf).triu_(1)\n",
    "        self.register_buffer('mask', mask, persistent=False)\n",
    "\n",
    "    def forward(self, x: Tensor, xa: Tensor, kv_cache: Optional[dict] = None):\n",
    "        offset = next(iter(kv_cache.values())).shape[1] if kv_cache else 0\n",
    "        positions = torch.arange(offset, offset + x.shape[-1], device=x.device)\n",
    "        x = self.token_embedding(x) + self.positional_embedding(positions)\n",
    "        x = x.to(xa.dtype)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, xa, mask=self.mask, kv_cache=kv_cache)\n",
    "\n",
    "        # for block in self.blocks:\n",
    "        #     x = checkpoint.checkpoint(block_forward, block, x, xa, self.mask, kv_cache)\n",
    "\n",
    "        x = self.ln(x)\n",
    "        logits = (x @ torch.transpose(self.token_embedding.weight.to(x.dtype), 0, 1)).float()\n",
    "        return logits\n",
    "\n",
    "\n",
    "# def block_forward(block, x, xa, mask, kv_cache):\n",
    "#     return block(x, xa, mask=mask, kv_cache=kv_cache)\n",
    "\n",
    "# class TextDecoder(nn.Module):\n",
    "#     def __init__(self, n_vocab: int, n_ctx: int, n_state: int, n_head: int, n_layer: int):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.token_embedding = nn.Embedding(n_vocab, n_state)\n",
    "#         self.positional_embedding = LearnedSinusoidalEmbeddings(n_ctx, n_state)\n",
    "#         self.rotary_emb = RotaryEmbedding(dim=n_state // n_head)\n",
    "        \n",
    "#         self.blocks: Iterable[ResidualAttentionBlock] = nn.ModuleList(\n",
    "#             [ResidualAttentionBlock(n_state, n_head, cross_attention=True) for _ in range(n_layer)]\n",
    "#         )\n",
    "#         self.ln = LayerNorm(n_state)\n",
    "\n",
    "#         mask = torch.empty(n_ctx, n_ctx).fill_(-np.inf).triu_(1)\n",
    "#         self.register_buffer('mask', mask, persistent=False)\n",
    "\n",
    "#     def forward(self, x: Tensor, xa: Tensor, kv_cache: Optional[dict] = None):\n",
    "#         offset = next(iter(kv_cache.values())).shape[1] if kv_cache else 0\n",
    "#         positions = torch.arange(offset, offset + x.shape[-1], device=x.device)\n",
    "#         x = self.token_embedding(x) + self.positional_embedding(positions)\n",
    "#         x = x.to(xa.dtype)\n",
    "\n",
    "#         for block in self.blocks:\n",
    "#             x = checkpoint.checkpoint(block_forward, block, x, xa, self.mask, kv_cache)\n",
    "\n",
    "#         x = self.ln(x)\n",
    "#         logits = (x @ torch.transpose(self.token_embedding.weight.to(x.dtype), 0, 1)).float()\n",
    "#         return logits\n",
    "\n",
    "    \n",
    "class LearnedSinusoidalEmbeddings(nn.Module): # sinusofeatures(n_ctx, n_state)\n",
    "    def __init__(self, n_ctx, n_state):\n",
    "        super().__init__()\n",
    "        self.n_ctx = n_ctx\n",
    "        self.n_state = n_state\n",
    "\n",
    "        # Initialize with sinusoidal embeddings\n",
    "        sinusoidal_embeddings = sinusofeatures(n_ctx, n_state)\n",
    "        self.positional_embeddings = nn.Parameter(sinusoidal_embeddings)\n",
    "\n",
    "    def forward(self, positions):\n",
    "        position_embeddings = self.positional_embeddings[positions]\n",
    "        return position_embeddings\n",
    "\n",
    "class Whisper(nn.Module):\n",
    "    def __init__(self, dims: ModelDimensions):\n",
    "        super().__init__()\n",
    "        self.dims = dims\n",
    "        self.encoder = AudioEncoder(\n",
    "            self.dims.n_mels,\n",
    "            self.dims.n_audio_ctx,\n",
    "            self.dims.n_audio_state,\n",
    "            self.dims.n_audio_head,\n",
    "            self.dims.n_audio_layer,\n",
    "        )\n",
    "        self.decoder = TextDecoder(\n",
    "            self.dims.n_vocab,\n",
    "            self.dims.n_text_ctx,\n",
    "            self.dims.n_text_state,\n",
    "            self.dims.n_text_head,\n",
    "            self.dims.n_text_layer,\n",
    "        )\n",
    "\n",
    "        all_heads = torch.zeros(\n",
    "            self.dims.n_text_layer, self.dims.n_text_head, dtype=torch.bool\n",
    "        )\n",
    "        all_heads[self.dims.n_text_layer // 2 :] = True\n",
    "        self.register_buffer('alignment_heads', all_heads.to_sparse(), persistent=False)\n",
    "\n",
    "    def set_alignment_heads(self, dump: bytes):\n",
    "        array = np.frombuffer(\n",
    "            gzip.decompress(base64.b85decode(dump)), dtype=bool\n",
    "        ).copy()\n",
    "        mask = torch.from_numpy(array).reshape(\n",
    "            self.dims.n_text_layer, self.dims.n_text_head\n",
    "        )\n",
    "        self.register_buffer('alignment_heads', mask.to_sparse(), persistent=False)\n",
    "\n",
    "    def embed_audio(self, mel: torch.Tensor):\n",
    "        return self.encoder(mel)\n",
    "\n",
    "    def logits(self, tokens: torch.Tensor, audio_features: torch.Tensor):\n",
    "        return self.decoder(tokens, audio_features)\n",
    "\n",
    "    def forward(self, mel: torch.Tensor, tokens: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        audio_features = self.encoder(mel)\n",
    "        logits = self.decoder(tokens, audio_features)\n",
    "        return {\"logits\": logits}\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    @property\n",
    "    def is_multilingual(self):\n",
    "        return self.dims.n_vocab >= 51865\n",
    "\n",
    "    @property\n",
    "    def num_languages(self):\n",
    "        return self.dims.n_vocab - 51765 - int(self.is_multilingual)\n",
    "\n",
    "    def install_kv_cache_hooks(self, cache: Optional[dict] = None):\n",
    "        cache = {**cache} if cache is not None else {}\n",
    "        hooks = []\n",
    "\n",
    "        def save_to_cache(module, _, output):\n",
    "            if module not in cache or output.shape[1] > self.dims.n_text_ctx:\n",
    "                # save as-is, for the first token or cross attention\n",
    "                cache[module] = output\n",
    "            else:\n",
    "                cache[module] = torch.cat([cache[module], output], dim=1).detach()\n",
    "            return cache[module]\n",
    "\n",
    "        def install_hooks(layer: nn.Module):\n",
    "            if isinstance(layer, MultiHeadAttention):\n",
    "                hooks.append(layer.key.register_forward_hook(save_to_cache))\n",
    "                hooks.append(layer.value.register_forward_hook(save_to_cache))\n",
    "\n",
    "        self.decoder.apply(install_hooks)\n",
    "        return cache, hooks\n",
    "\n",
    "    detect_language = detect_language_function\n",
    "    transcribe = transcribe_function\n",
    "    decode = decode_function\n",
    "\n",
    "dimensions = ModelDimensions(\n",
    "    n_mels=128, \n",
    "    n_audio_ctx=1500, \n",
    "    n_audio_state=1280, \n",
    "    n_audio_head=20, \n",
    "    n_audio_layer=26, \n",
    "    n_vocab=51866, \n",
    "    n_text_ctx=448, \n",
    "    n_text_state=1280, \n",
    "    n_text_head=16, \n",
    "    n_text_layer=4\n",
    "    )\n",
    "\n",
    "model = Whisper(dimensions).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = WhisperForConditionalGeneration.from_pretrained('openai/whisper-large-v3-turbo')\n",
    "pretrained_state_dict = pretrained_model.state_dict()\n",
    "\n",
    "model = Whisper(dimensions).cuda()\n",
    "model_state_dict = model.state_dict()\n",
    "\n",
    "def transfer_layer(src_name, tgt_name):\n",
    "    if src_name in pretrained_state_dict and tgt_name in model_state_dict:\n",
    "        src_tensor = pretrained_state_dict[src_name]\n",
    "        tgt_tensor = model_state_dict[tgt_name]\n",
    "        print(f'Transferring layer {src_name} to {tgt_name}')\n",
    "        print(f'Source shape: {src_tensor.shape}, Target shape: {tgt_tensor.shape}')\n",
    "        tgt_tensor.copy_(src_tensor)\n",
    "\n",
    "\n",
    "# Transfer convolutional layers\n",
    "transfer_layer('model.encoder.conv1.weight', 'encoder.conv1.weight')\n",
    "transfer_layer('model.encoder.conv1.bias', 'encoder.conv1.bias')\n",
    "transfer_layer('model.encoder.conv2.weight', 'encoder.conv2.weight')\n",
    "transfer_layer('model.encoder.conv2.bias', 'encoder.conv2.bias')\n",
    "\n",
    "# Transfer positional embeddings\n",
    "# transfer_layer('model.encoder.embed_positions.weight', 'encoder.positional_embedding.weight')\n",
    "# transfer_layer('model.decoder.embed_positions.weight', 'decoder.positional_embedding.weight')\n",
    "\n",
    "# Transfer layer norms\n",
    "transfer_layer('model.encoder.layer_norm.weight', 'encoder.ln_post.weight')\n",
    "transfer_layer('model.encoder.layer_norm.bias', 'encoder.ln_post.bias')\n",
    "transfer_layer('model.decoder.layer_norm.weight', 'decoder.ln.weight')\n",
    "transfer_layer('model.decoder.layer_norm.bias', 'decoder.ln.bias')\n",
    "\n",
    "# Transfer token embeddings\n",
    "transfer_layer('model.decoder.embed_tokens.weight', 'decoder.token_embedding.weight') # tokenizer\n",
    "\n",
    "# Transfer encoder and decoder block layers\n",
    "for i in range(6):\n",
    "    transfer_layer(f'model.encoder.layers.{i}.self_attn.k_proj.weight', f'encoder.blocks.{i}.attn.key.weight')\n",
    "    transfer_layer(f'model.encoder.layers.{i}.self_attn.v_proj.weight', f'encoder.blocks.{i}.attn.value.weight')\n",
    "    transfer_layer(f'model.encoder.layers.{i}.self_attn.q_proj.weight', f'encoder.blocks.{i}.attn.query.weight')\n",
    "    transfer_layer(f'model.encoder.layers.{i}.self_attn.out_proj.weight', f'encoder.blocks.{i}.attn.out.weight')\n",
    "    transfer_layer(f'model.encoder.layers.{i}.self_attn_layer_norm.weight', f'encoder.blocks.{i}.attn_ln.weight')\n",
    "    transfer_layer(f'model.encoder.layers.{i}.self_attn_layer_norm.bias', f'encoder.blocks.{i}.attn_ln.bias')\n",
    "    transfer_layer(f'model.encoder.layers.{i}.fc1.weight', f'encoder.blocks.{i}.mlp.0.weight')\n",
    "    transfer_layer(f'model.encoder.layers.{i}.fc1.bias', f'encoder.blocks.{i}.mlp.0.bias')\n",
    "    transfer_layer(f'model.encoder.layers.{i}.fc2.weight', f'encoder.blocks.{i}.mlp.2.weight')\n",
    "    transfer_layer(f'model.encoder.layers.{i}.fc2.bias', f'encoder.blocks.{i}.mlp.2.bias')\n",
    "    transfer_layer(f'model.encoder.layers.{i}.final_layer_norm.weight', f'encoder.blocks.{i}.mlp_ln.weight')\n",
    "    transfer_layer(f'model.encoder.layers.{i}.final_layer_norm.bias', f'encoder.blocks.{i}.mlp_ln.bias')\n",
    "    transfer_layer(f'model.decoder.layers.{i}.self_attn.k_proj.weight', f'decoder.blocks.{i}.attn.key.weight')\n",
    "    transfer_layer(f'model.decoder.layers.{i}.self_attn.v_proj.weight', f'decoder.blocks.{i}.attn.value.weight')\n",
    "    transfer_layer(f'model.decoder.layers.{i}.self_attn.q_proj.weight', f'decoder.blocks.{i}.attn.query.weight')\n",
    "    transfer_layer(f'model.decoder.layers.{i}.self_attn.out_proj.weight', f'decoder.blocks.{i}.attn.out.weight')\n",
    "    transfer_layer(f'model.decoder.layers.{i}.self_attn_layer_norm.weight', f'decoder.blocks.{i}.attn_ln.weight')\n",
    "    transfer_layer(f'model.decoder.layers.{i}.self_attn_layer_norm.bias', f'decoder.blocks.{i}.attn_ln.bias')\n",
    "    transfer_layer(f'model.decoder.layers.{i}.encoder_attn.k_proj.weight', f'decoder.blocks.{i}.cross_attn.key.weight')\n",
    "    transfer_layer(f'model.decoder.layers.{i}.encoder_attn.v_proj.weight', f'decoder.blocks.{i}.cross_attn.value.weight')\n",
    "    transfer_layer(f'model.decoder.layers.{i}.encoder_attn.q_proj.weight', f'decoder.blocks.{i}.cross_attn.query.weight')\n",
    "    transfer_layer(f'model.decoder.layers.{i}.encoder_attn.out_proj.weight', f'decoder.blocks.{i}.cross_attn.out.weight')\n",
    "    transfer_layer(f'model.decoder.layers.{i}.encoder_attn_layer_norm.weight', f'decoder.blocks.{i}.cross_attn_ln.weight')\n",
    "    transfer_layer(f'model.decoder.layers.{i}.encoder_attn_layer_norm.bias', f'decoder.blocks.{i}.cross_attn_ln.bias')\n",
    "    transfer_layer(f'model.decoder.layers.{i}.fc1.weight', f'decoder.blocks.{i}.mlp.0.weight')\n",
    "    transfer_layer(f'model.decoder.layers.{i}.fc1.bias', f'decoder.blocks.{i}.mlp.0.bias')\n",
    "    transfer_layer(f'model.decoder.layers.{i}.fc2.weight', f'decoder.blocks.{i}.mlp.2.weight')\n",
    "    transfer_layer(f'model.decoder.layers.{i}.fc2.bias', f'decoder.blocks.{i}.mlp.2.bias')\n",
    "    transfer_layer(f'model.decoder.layers.{i}.final_layer_norm.weight', f'decoder.blocks.{i}.mlp_ln.weight')\n",
    "    transfer_layer(f'model.decoder.layers.{i}.final_layer_norm.bias', f'decoder.blocks.{i}.mlp_ln.bias')\n",
    "\n",
    "# Load the updated state dict into the custom model\n",
    "model.load_state_dict(model_state_dict)\n",
    "\n",
    "# Now you can proceed with training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "tokenizer = WhisperTokenizer.from_pretrained('openai/whisper-medium')\n",
    "csv_file = 'D:/proj/datasets/gvj/trimmed/metadata.csv'\n",
    "audio_dir = 'D:/proj/datasets/gvj/trimmed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class WhisperDataCollatorWithPadding:\n",
    "    tokenizer: Any\n",
    "    padding_value: int = -100\n",
    "\n",
    "    def __call__(self, features):\n",
    "        input_features, labels, dec_input_features = [], [], []\n",
    "\n",
    "        for f in features:\n",
    "            input_features.append(f[\"input_features\"])\n",
    "            labels.append(f[\"labels\"])\n",
    "            dec_input_features.append(f[\"dec_input_features\"])\n",
    "\n",
    "        input_features = torch.stack(input_features)\n",
    "\n",
    "        # Find the maximum length for padding\n",
    "        max_label_len = max(len(label) for label in labels)\n",
    "        max_dec_input_len = max(len(dec_input) for dec_input in dec_input_features)\n",
    "\n",
    "        # Pad labels and dec_input_features to the max length\n",
    "        labels = [np.pad(label, (0, max_label_len - len(label)), 'constant', constant_values=self.padding_value) for label in labels]\n",
    "        dec_input_features = [np.pad(dec_input, (0, max_dec_input_len - len(dec_input)), 'constant', constant_values=self.tokenizer.pad_token_id) for dec_input in dec_input_features]\n",
    "\n",
    "        # Convert lists of numpy arrays to a single numpy array before converting to tensors\n",
    "        labels = np.array(labels)\n",
    "        dec_input_features = np.array(dec_input_features)\n",
    "\n",
    "        batch = {\n",
    "            \"input_features\": input_features,\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "            \"dec_input_features\": torch.tensor(dec_input_features, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "        return batch\n",
    "\n",
    "# Create the data collator instance\n",
    "collate_fn = WhisperDataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio.transforms as at\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def load_wave(wave_path, sample_rate: int = 16000) -> torch.Tensor:\n",
    "    waveform, sr = torchaudio.load(wave_path, normalize=True)\n",
    "    if sample_rate != sr:\n",
    "        waveform = at.Resample(sr, sample_rate)(waveform)\n",
    "    return waveform\n",
    "\n",
    "class CustomAudioDataset(Dataset):\n",
    "    def __init__(self, csv_file, audio_dir, tokenizer, sample_rate=16000):\n",
    "        self.audio_dir = audio_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sample_rate = sample_rate\n",
    "        self.samples = []\n",
    "\n",
    "        with open(csv_file, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader)  # Skip header row if it exists\n",
    "            for row in reader:\n",
    "                audio_path, label = row[0], row[1]\n",
    "                self.samples.append((audio_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path, label = self.samples[idx]\n",
    "        audio_path = f'{self.audio_dir}/{audio_path}'\n",
    "\n",
    "        audio = load_wave(audio_path, sample_rate=self.sample_rate)\n",
    "        audio = whisper.pad_or_trim(audio.flatten())\n",
    "        input_features = whisper.log_mel_spectrogram(audio, n_mels=128)\n",
    "\n",
    "        label_tokens = [self.tokenizer.bos_token_id] + self.tokenizer.encode(label) + [self.tokenizer.eos_token_id]\n",
    "        dec_input_features = label_tokens[:-1]\n",
    "        labels = label_tokens[1:]\n",
    "\n",
    "        return {\n",
    "            'input_features': input_features,\n",
    "            'dec_input_features': dec_input_features,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "dataset = CustomAudioDataset(csv_file, audio_dir, tokenizer)\n",
    "\n",
    "def train_val_dataset(dataset, val_split=0.001):\n",
    "    train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_split)\n",
    "    datasets = {}\n",
    "    datasets['train'] = Subset(dataset, train_idx)\n",
    "    datasets['val'] = Subset(dataset, val_idx)\n",
    "    return datasets\n",
    "\n",
    "datasets = train_val_dataset(dataset, val_split=0.001)\n",
    "train_dataset = datasets['train']\n",
    "eval_dataset = datasets['val']\n",
    "\n",
    "def train_dataloader():   \n",
    "    dataset = train_dataset\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=2,\n",
    "        drop_last=True, \n",
    "        shuffle=True, \n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "def eval_dataloader():\n",
    "    dataset = eval_dataset\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=2,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_fn\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"cer\")\n",
    "wakati = MeCab.Tagger(\"-Owakati\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_features = pred.predictions\n",
    "    label_features = pred.label_features\n",
    "    label_features[label_features == -100] = tokenizer.pad_token_id\n",
    "    pred_str = tokenizer.batch_decode(pred_features, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_features, skip_special_tokens=True)\n",
    "    \n",
    "    pred_str_nj = [wakati.parse(pred) for pred in pred_str] \n",
    "    label_str_nj = [wakati.parse(label) for label in label_str] \n",
    "    pred_str_nj = [pred_str_nj[i] for i in range(len(pred_str_nj)) if len(label_str_nj[i]) > 0]\n",
    "    label_str_nj = [\n",
    "        label_str_nj[i]\n",
    "        for i in range(len(label_str_nj))\n",
    "        if len(label_str_nj[i]) > 0]\n",
    "    \n",
    "    pred_str_neo = [neologdn.normalize(pred) for pred in pred_str] \n",
    "    label_str_neo = [neologdn.normalize(label) for label in label_str] \n",
    "    pred_str_neo = [pred_str_neo[i] for i in range(len(pred_str_neo)) if len(label_str_neo[i]) > 0]\n",
    "    label_str_neo = [\n",
    "        label_str_neo[i]\n",
    "        for i in range(len(label_str_neo))\n",
    "        if len(label_str_neo[i]) > 0]\n",
    "    \n",
    "    cer = 100 * metric.compute(predictions=pred_str, references=label_str) # no normalization\n",
    "    cer_mecab = 100 * metric.compute(predictions=pred_str_nj, references=label_str_nj) # mecab normalization\n",
    "    cer_neo = 100 * metric.compute(predictions=pred_str_neo, references=label_str_neo) # \n",
    "    return {\"cer\": cer,  \"cer_mecab\": cer_mecab, \"cer_neo\": cer_neo}#, \"blue\": blue, \"accuracy\": accuracy} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 86\u001b[0m\n\u001b[0;32m     83\u001b[0m eval_loader \u001b[38;5;241m=\u001b[39m eval_dataloader()\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Train the model with profiling\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m \u001b[43mtrain_with_profiling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclear_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 32\u001b[0m, in \u001b[0;36mtrain_with_profiling\u001b[1;34m(model, train_dataloader, eval_dataloader, optimizer, criterion, num_epochs, device, accumulation_steps, eval_steps, clear_cache)\u001b[0m\n\u001b[0;32m     29\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Perform optimization step every accumulation_steps\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_with_profiling(model, train_dataloader, eval_dataloader, optimizer, criterion, num_epochs=3, device='cuda', accumulation_steps=4, eval_steps=100, clear_cache=True):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    metric = evaluate.load(\"cer\")\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "        with record_function(\"model_training\"):\n",
    "            for epoch in range(num_epochs):\n",
    "                total_loss = 0\n",
    "                optimizer.zero_grad()\n",
    "                progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "                for step, batch in enumerate(progress_bar):\n",
    "                    input_features = batch['input_features'].to(device)\n",
    "                    labels = batch['labels'].long().to(device)\n",
    "                    dec_input_features = batch['dec_input_features'].to(device)\n",
    "\n",
    "                    # Forward pass\n",
    "                    encoder_outputs = model.encoder(input_features)\n",
    "                    decoder_outputs = model.decoder(dec_input_features, encoder_outputs)\n",
    "\n",
    "                    logits = decoder_outputs.view(-1, decoder_outputs.size(-1))\n",
    "                    loss = criterion(logits, labels.view(-1))\n",
    "\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "                    # Backward pass\n",
    "                    loss.backward()\n",
    "\n",
    "                    # Perform optimization step every accumulation_steps\n",
    "                    if (step + 1) % accumulation_steps == 0:\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        # Optionally clear cache\n",
    "                        if clear_cache:\n",
    "                            torch.cuda.empty_cache()\n",
    "\n",
    "                    # Evaluate metrics every eval_steps\n",
    "                    if (step + 1) % eval_steps == 0:\n",
    "                        model.eval()\n",
    "                        all_predictions = []\n",
    "                        all_labels = []\n",
    "                        for eval_batch in eval_dataloader:\n",
    "                            eval_input_features = eval_batch['input_features'].to(device)\n",
    "                            eval_labels = eval_batch['labels'].long().to(device)\n",
    "                            eval_dec_input_features = eval_batch['dec_input_features'].to(device)\n",
    "\n",
    "                            with torch.no_grad():\n",
    "                                encoder_outputs = model.encoder(eval_input_features)\n",
    "                                decoder_outputs = model.decoder(eval_dec_input_features, encoder_outputs)\n",
    "\n",
    "                            all_predictions.append(decoder_outputs)\n",
    "                            all_labels.append(eval_labels)\n",
    "\n",
    "                        all_predictions = torch.cat([torch.argmax(p, dim=-1) for p in all_predictions], dim=0)\n",
    "                        all_labels = torch.cat(all_labels, dim=0)\n",
    "                        metrics = compute_metrics({'predictions': all_predictions, 'label_features': all_labels})\n",
    "                        print(f\"Metrics at step {step + 1}, epoch {epoch + 1}: {metrics}\")\n",
    "                        model.train()\n",
    "\n",
    "                    # Update progress bar\n",
    "                    progress_bar.set_postfix(loss=total_loss / (step + 1), iters_per_sec=progress_bar.format_dict[\"rate\"])\n",
    "\n",
    "                if (step + 1) % accumulation_steps != 0:\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                print(f'Epoch {epoch + 1}, Loss: {total_loss / len(train_dataloader)}')\n",
    "\n",
    "    print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "\n",
    "# Initialize optimizer and loss function\n",
    "optimizer = optim.Adafactor(model.parameters(), lr=5e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = train_dataloader()\n",
    "eval_loader = eval_dataloader()\n",
    "\n",
    "# Train the model with profiling\n",
    "train_with_profiling(model, train_loader, eval_loader, optimizer, criterion, num_epochs=3, device='cuda', accumulation_steps=4, eval_steps=100, clear_cache=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize Adafactor optimizer\n",
    "# optimizer = Adafactor(\n",
    "#     model.parameters(), \n",
    "#     scale_parameter=True, \n",
    "#     relative_step=True, \n",
    "#     warmup_init=True, \n",
    "#     lr=None\n",
    "# )\n",
    "\n",
    "# def train_combined(model, dataloader, optimizer, criterion, num_epochs=3, device='cuda', accumulation_steps=4):\n",
    "#     model.to(device)\n",
    "#     model.train()\n",
    "#     scaler = GradScaler()\n",
    "\n",
    "#     with profiler.profile(activities=[profiler.ProfilerActivity.CPU, profiler.ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "#         with profiler.record_function(\"model_training\"):\n",
    "#             for epoch in range(num_epochs):\n",
    "#                 total_loss = 0\n",
    "#                 optimizer.zero_grad()\n",
    "#                 progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "#                 for i, batch in enumerate(progress_bar):\n",
    "#                     input_features = batch['input_features'].to(device)\n",
    "#                     labels = batch['labels'].long().to(device)\n",
    "#                     dec_input_features = batch['dec_input_features'].to(device)\n",
    "\n",
    "#                     with autocast(device_type='cuda'):\n",
    "#                         # Forward pass\n",
    "#                         encoder_outputs = model.encoder(input_features)\n",
    "#                         decoder_outputs = model.decoder(dec_input_features, encoder_outputs)\n",
    "\n",
    "#                         logits = decoder_outputs.view(-1, decoder_outputs.size(-1))\n",
    "#                         loss = criterion(logits, labels.view(-1))\n",
    "\n",
    "#                     total_loss += loss.item()\n",
    "\n",
    "#                     # Backward pass\n",
    "#                     scaler.scale(loss).backward()\n",
    "\n",
    "#                     # Perform optimization step every accumulation_steps\n",
    "#                     if (i + 1) % accumulation_steps == 0:\n",
    "#                         scaler.step(optimizer)\n",
    "#                         scaler.update()\n",
    "#                         optimizer.zero_grad()\n",
    "\n",
    "#                     # Update progress bar\n",
    "#                     progress_bar.set_postfix(loss=total_loss / (i + 1), iters_per_sec=progress_bar.format_dict[\"rate\"])\n",
    "\n",
    "#                 if (i + 1) % accumulation_steps != 0:\n",
    "#                     scaler.step(optimizer)\n",
    "#                     scaler.update()\n",
    "#                     optimizer.zero_grad()\n",
    "\n",
    "#                 print(f'Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}')\n",
    "\n",
    "#     print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "\n",
    "# # Initialize optimizer and loss function\n",
    "# optimizer = optim.Adafactor(model.parameters(), lr=5e-5)\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "# # Train the model with all combined features\n",
    "# train_combined(model, dataloader, optimizer, criterion, num_epochs=3, device='cuda', accumulation_steps=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_val_dataset(dataset, val_split=0.1):\n",
    "#     train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_split)\n",
    "#     datasets = {}\n",
    "#     datasets['train'] = Subset(dataset, train_idx)\n",
    "#     datasets['val'] = Subset(dataset, val_idx)\n",
    "#     return datasets\n",
    "\n",
    "# datasets = train_val_dataset(dataset, val_split=0.1)\n",
    "# train_dataset = datasets['train']\n",
    "# eval_dataset = datasets['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def map_dataset(map):\n",
    "#     audio_path = map[\"audio\"]\n",
    "#     waveform, sample_rate = torchaudio.load(audio_path[\"array\"])\n",
    "\n",
    "#     # Pad or trim the waveform to fit the model's input requirements\n",
    "#     waveform = whisper.pad_or_trim(waveform)\n",
    "\n",
    "#     # Compute log-Mel spectrogram\n",
    "#     log_mel = whisper.log_mel_spectrogram(waveform)\n",
    "    \n",
    "#     labels = tokenizer.encode(map[\"sentence\"], return_tensors=\"pt\", padding=\"max_length\", max_length=128, truncation=True).squeeze(0)\n",
    "\n",
    "#     return {\n",
    "#         'input_features': log_mel,\n",
    "#         'labels': labels\n",
    "#     }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
